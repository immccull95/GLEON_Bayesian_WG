State-space model - revised by S. LaDeau (11/2017) from the EcoForecast Activity by Michael Dietze, with reference "Ecological Forecasting", chapter 8
========================================================

The data used for this example are from summer weekly(ish) Gloetrichia echinulata (Gloeo.) sampling at 4 locations in Lake Sunapee, NH. The data are provided by Kathryn Cottingham, and should not be used without permission outside this workshop.

This activity will explore the state-space framework for modeling time-series and spatial data sets. It is based on separating the process model, which describes how the system evolves in time or space, from the observation (data) model. The state-space model gets its name because the model estimates that true value of the underlying **latent** state variables.



```{r}
install.packages("tidyverse")
library(readxl)
library(rjags)
library(moments)
```

Begin with data from one location:

```{r}

data=read_csv("~/GitHub/GLEON_Bayesian_WG/Datasets/Sunapee/SummarizedData/All_Sites_Gloeo_light_wtrtemp.csv")

Midge=subset(data, site=="Midge")
Coffin=subset(data, site=="Coffin")
Fichter=subset(data, site=="Fichter")
Newbury=subset(data, site=="Newbury")

#Choose correct site

dat=Midge
#dat=Coffin
#dat=Fichter
#dat=Newbury

str(dat)

time=as.character(dat$date)
#times<-as.Date(time)

##look at response variable - what data distributions are appropriate?
y<-round(dat$totalperL*141.3707) #converting colonies per Liter to count data: volume of 2, ~1 m net tows
hist(y)  
N=length(y)
range(y)
```

The large spread in the data AND the zero records for y present issues for choosing appropriate data model from standard distributions. For the purposes of this workshop we'll start with the common (but not necessarily desirable) practice of adding a small number to y so that all y > 0. Thus, we're assuming that there is some lower bound detection error and that gloeo are in fact present at some level at all samples. This assumption may be correct - or not- but should be carefully evaluated for any use outside this activity. We have introduced a systematic error or bias.

Better way:
Need to model probablility of 0 - poisson distribution?

```{r}
plot(times,y,type='l',ylab="gloeo colonies",lwd=2)
   lines(times,y.,lty=2,col=2) ##compare with the unadulterated data

```

Next define the JAGS code. The code itself has three components, the data model, the process model, and the priors. The data model relates the observed data, y, at any time point to the latent variable, x. For this example we'll assume that the observation model just consists of Gaussian observation error. The process model relates the state of the system at one point in time to the state one time step ahead. In this case we'll start with the simplest possible process model, a random walk, which just consists of Gaussian process error centered around the current value of the system. Finally, we need to define priors for all stochastic variables, including the initial value of x, the process error, and the observation error.

```{r}
RandomWalk = "

  #### Data Model
  model{
  for(i in 1:N){
    #this fits the blended model to your observed data. 
    y[i] ~ dpois(m[i])
    
    #This blends the poisson and zero inflation models
    m[i] <- exp(mu[i])*b[i] + 1E-10 #only when comparing them to the data, we need to exponentiate
    
    #this is the bernoulli outcome of the zero inflation
    b[i] ~ dbern(theta)
  }

   #### Process Model
  for(i in 2:N){
    mu[i]~dnorm(mu[i-1],tau_add) #mu's here are on log scale
  }
  
  #### Priors
  mu[1] ~ dnorm(x_ic,tau_ic) 
  tau_add ~ dgamma(a_add,r_add)
  theta~ dbeta(alpha, beta)
}
"
```

Next we need to define the data and priors as a list. For this analysis we'll work with the log of y since the zero-bound on the index and the magnitudes of the changes appear much closer to a log-normal distribution than to a normal. [This only works for y>0]. The priors on error terms are standard, non-informative and the initial condition (x_ic) is parameterized to be within the range of known measurements.

```{r}
data <- list(y=y,N=length(y),x_ic=log(0.1),tau_ic=100,a_add=.001,r_add=.001, alpha=1, beta=1)
```

Next we need to define the initial state of the model's parameters for each chain in the MCMC. The overall initialization is stored as a list the same length as the number of chains, where each chain is passed a list of the initial values for each parameter. Unlike the definition of the priors, which had to be done independent of the data, the inidialization of the MCMC is allowed (and even encouraged) to use the data. However, each chain should be started from different initial conditions. We handle this below by basing the initial conditions for each chain off of a different random sample of the original data. 

```{r}
nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp)))
}
```

Now that we've defined the model, the data, and the initialization, we need to send all this info to JAGS, which will return the JAGS model object.

```{r}
j.model   <- jags.model (file = textConnection(RandomWalk),
                             data = data,
                             inits = init,
                             n.chains = 3)
```

Next, given the defined JAGS model, we'll want to take a few samples from the MCMC chain and assess when the model has converged. To take samples from the MCMC object we'll need to tell JAGS what variables to track and how many samples to take.

```{r}
## burn-in
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("tau_add", "theta"),
                                n.iter = 1000)
plot(jags.out)
```


The model chains seem to converge fairly rapidly for both error terms. Since rjags returns the samples as a CODA object, we can use any of the diagnositics in the R *coda* library to test for convergence, summarize the output, or visualize the chains.

Now that the model has converged we'll want to take a much larger sample from the MCMC and include the full vector of X's in the output.

```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("tau_add", "mu"),
                                n.iter = 5000)

DIC=dic.samples(j.model, n.iter=1000) #improved with lognormal- NOTE! 
DIC

```

Given the full joint posterior samples, we're next going to visualize the output by just looking at the 95% credible interval of the timeseries of X's and compare that to the observed Y's. To do so we'll convert the coda output into a matrix and then calculate the quantiles. Looking at colnames(out) will show you that the first two columns are `tau_add` and `tau_obs`, so we calculate the CI starting from the 3rd column. We also transform the samples back from the log domain to the linear domain.

```{r}
time.rng = c(1,length(times)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out <- as.matrix(jags.out)
mus=grep("mu", colnames(out))
mu = exp(out[,mus])
ci <- apply(exp(out[,mus]),2,quantile,c(0.025,0.5,0.975))

plot(times,ci[2,],type='n',ylim=range(y+.01,na.rm=TRUE), log="y", ylab="colonies/L",xlim=times[time.rng])

ciEnvelope(times,ci[1,],ci[3,],col="lightBlue")
points(times,y,pch="+",cex=0.5)
```

one step ahead prediction: can plot using code above; plot out-of-sample predictions first because wide CIs; calculate means and plot predicted vs. observed
```{r}
nsamp = 500
out <- as.matrix(jags.out)
samp <- sample.int(nrow(out),nsamp)

## sample initial conditions
mus=grep("mu", colnames(out))
mu = out[samp,mus] 
times=c(1:length(mus))

## sample tau
tau = out[samp,grep("tau",colnames(out))]

pred <- matrix(NA,nrow=nsamp,ncol=ncol(mu))
TS1 <- NULL

for (t in 2:ncol(mu)){
pred[,t] = rnorm(nsamp,mu[,t-1],tau) #exponentiate these before comparing to data, because mu on log scale
}

#Visualization

time.rng = c(1,length(times)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out <- as.matrix(jags.out)
mus=grep("mu", colnames(out))
mu = exp(out[,mus])
ci <- apply(exp(out[,mus]),2,quantile,c(0.025,0.5,0.975))
pi <- apply(exp(pred),2,quantile,c(0.025,0.5,0.975), na.rm=TRUE)

plot(times,ci[2,],type='n',ylim=range(y+.01,na.rm=TRUE), log="y", ylab="colonies/L",xlim=times[time.rng])
ciEnvelope(times,pi[1,],pi[3,],col="Green")
ciEnvelope(times,ci[1,],ci[3,],col="lightBlue")
points(times,y,pch="+",cex=0.5)

```

#Checks

```{r}

#mu vs. preds

diffs <- matrix(NA,nrow=nsamp,ncol=ncol(mu))

for(i in 1:nsamp){
  for(j in 1:ncol(mu)){
diffs[i,j]=mu[i,j]-pred[i,j]
  }
}
row_sums=rowSums(diffs, na.rm=TRUE) #NA.rm required because first column is NA
pred_sum=sum(row_sums)

#y vs. preds

obs_diff= vector(mode="numeric", length=0)

for(i in 1:ncol(pred)){
  obs_diff[i]=mean(pred[,i])-y[i]
}

obspred_sum=sum(obs_diff, na.rm=TRUE)
```

