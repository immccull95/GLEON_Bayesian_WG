<<<<<<< HEAD
State-space model - revised by S. LaDeau (11/2017) from the EcoForecast Activity by Michael Dietze, with reference "Ecological Forecasting", chapter 8
========================================================

The data used for this example are from summer weekly(ish) Gloetrichia echinulata (Gloeo.) sampling at 4 locations in Lake Sunapee, NH. The data are provided by Kathryn Cottingham, and should not be used without permission outside this workshop.

This activity will explore the state-space framework for modeling time-series and spatial data sets. It is based on separating the process model, which describes how the system evolves in time or space, from the observation (data) model. The state-space model gets its name because the model estimates that true value of the underlying **latent** state variables.



```{r}
library(tidyverse)
library(readxl)
library(rjags)
library(moments)
library(geosphere)
```

Begin with data from one location:

```{r}

fielddata=read_csv("~/GitHub/GLEON_Bayesian_WG/Datasets/Sunapee/SummarizedData/All_Sites_Gloeo_light_wtrtemp.csv") %>%
  filter(year >= 2007 & year <= 2014)

Midge=subset(fielddata, site=="Midge")
Coffin=subset(fielddata, site=="Coffin")
Fichter=subset(fielddata, site=="Fichter")
Newbury=subset(fielddata, site=="Newbury")

#Choose correct site

#dat=Midge
#dat=Coffin
#dat=Fichter
dat=Newbury

str(dat)
times <- dat$date

##look at response variable - what data distributions are appropriate?
y<-round(dat$totalperL*141.3707) #converting colonies per Liter to count data: volume of 2, ~1 m net tows
hist(y)  
N=length(y)
range(y)
```

The large spread in the data AND the zero records for y present issues for choosing appropriate data model from standard distributions. For the purposes of this workshop we'll start with the common (but not necessarily desirable) practice of adding a small number to y so that all y > 0. Thus, we're assuming that there is some lower bound detection error and that gloeo are in fact present at some level at all samples. This assumption may be correct - or not- but should be carefully evaluated for any use outside this activity. We have introduced a systematic error or bias.

Better way:
Need to model probablility of 0 - poisson distribution?

```{r}
plot(times,y,type='l',ylab="gloeo colonies",lwd=2)

```

Next define the JAGS code. The code itself has three components, the data model, the process model, and the priors. The data model relates the observed data, y, at any time point to the latent variable, x. For this example we'll assume that the observation model just consists of Gaussian observation error. The process model relates the state of the system at one point in time to the state one time step ahead. In this case we'll start with the simplest possible process model, a random walk, which just consists of Gaussian process error centered around the current value of the system. Finally, we need to define priors for all stochastic variables, including the initial value of x, the process error, and the observation error.

```{r}
RandomWalk = "

  #### Data Model
  model{
  for(i in 1:N){
    #this fits the blended model to your observed data. 
    y[i] ~ dpois(m[i])
    
    #exponentiate to get appropriate input to poisson
    m[i] <- exp(mu[i]) 
  }

   #### Process Model
  for(i in 2:N){
    mu[i]~dnorm(mu[i-1],tau_add) #mu's here are on log scale
  }
  
  #### Priors
  mu[1] ~ dnorm(x_ic,tau_ic) 
  tau_add ~ dgamma(a_add,r_add)
  }

"
```

Next we need to define the data and priors as a list. For this analysis we'll work with the log of y since the zero-bound on the index and the magnitudes of the changes appear much closer to a log-normal distribution than to a normal. [This only works for y>0]. The priors on error terms are standard, non-informative and the initial condition (x_ic) is parameterized to be within the range of known measurements.

```{r}
data <- list(y=y,N=length(y),x_ic=log(0.1),tau_ic=100,a_add=.001,r_add=.001)
```

Next we need to define the initial state of the model's parameters for each chain in the MCMC. The overall initialization is stored as a list the same length as the number of chains, where each chain is passed a list of the initial values for each parameter. Unlike the definition of the priors, which had to be done independent of the data, the inidialization of the MCMC is allowed (and even encouraged) to use the data. However, each chain should be started from different initial conditions. We handle this below by basing the initial conditions for each chain off of a different random sample of the original data. 

```{r}
nchain = 3
init <- list()
for(i in 1:nchain){
  y.samp = sample(y,length(y),replace=TRUE)
  init[[i]] <- list(tau_add=1/var(diff(y.samp)))
}
```

Now that we've defined the model, the data, and the initialization, we need to send all this info to JAGS, which will return the JAGS model object.

```{r}
j.model   <- jags.model (file = textConnection(RandomWalk),
                             data = data,
                             inits = init,
                             n.chains = 3)
```

Next, given the defined JAGS model, we'll want to take a few samples from the MCMC chain and assess when the model has converged. To take samples from the MCMC object we'll need to tell JAGS what variables to track and how many samples to take.

```{r}
## burn-in
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("tau_add"),
                                n.iter = 1000)
plot(jags.out)
```


The model chains seem to converge fairly rapidly for both error terms. Since rjags returns the samples as a CODA object, we can use any of the diagnositics in the R *coda* library to test for convergence, summarize the output, or visualize the chains.

Now that the model has converged we'll want to take a much larger sample from the MCMC and include the full vector of X's in the output.

```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("tau_add", "mu"),
                                n.iter = 5000)
save(jags.out, file = "C:/Users/Mary Lofton/Documents/GLEON/Bayesian/RandomWalk_Newbury.RData")

summary(jags.out)

DIC=dic.samples(j.model, n.iter=1000) #improved with lognormal- NOTE! 
DIC

```

Given the full joint posterior samples, we're next going to visualize the output by just looking at the 95% credible interval of the timeseries of X's and compare that to the observed Y's. To do so we'll convert the coda output into a matrix and then calculate the quantiles. Looking at colnames(out) will show you that the first two columns are `tau_add` and `tau_obs`, so we calculate the CI starting from the 3rd column. We also transform the samples back from the log domain to the linear domain.

```{r}
times <- as.Date(as.character(dat$date))
time.rng = c(1,length(times)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out <- as.matrix(jags.out)
mus=grep("mu", colnames(out))
mu = exp(out[,mus])
ci <- apply(exp(out[,mus]),2,quantile,c(0.025,0.5,0.975))

plot(times,ci[2,],type='n',ylim=range(y+.01,na.rm=TRUE), log="y", ylab="observed Gloeo colonies",xlim=times[time.rng])

ciEnvelope(times,ci[1,],ci[3,],col="lightBlue")
points(times,y,pch="+",cex=0.5)
```

one step ahead prediction: can plot using code above; plot out-of-sample predictions first because wide CIs; calculate means and plot predicted vs. observed
```{r}
#How many times do you want to sample to get predictive interval for each sampling day?
nsamp = 500

#Calculate prediction intervals for one timestep ahead
#your latent states must be called "mu" in the model!

out <- as.matrix(jags.out)
samp <- sample.int(nrow(out),nsamp)

## sample initial conditions
mus=grep("mu", colnames(out))
mu = out[samp,mus] 
times=c(1:length(mus))

## sample parameters - ADD IN THE PARAMETERS FROM YOUR MODEL HERE
tau = out[samp,grep("tau",colnames(out))]

pred <- matrix(NA,nrow=nsamp,ncol=ncol(mu))
pred_obs <- matrix(NA, nrow=nsamp, ncol=ncol(mu))

for (t in 2:ncol(mu)){
  
#Sample to get predictive interval for latent states based on process model
pred[,t] = rnorm(nsamp,mu[,t-1],tau) #exponentiate these before comparing to data, because mu on log scale

#exponentiate to get appropriate input for poisson
m <- exp(pred[,t])

#this fits the blended model to your observed data.
pred_obs[,t] = rpois(nsamp, m)

}


```

#Diagnostic Visualization
```{r}

#Visualization

time.rng = c(1,length(times)) ## adjust to zoom in and out
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
out <- as.matrix(jags.out)
mus=grep("mu", colnames(out))
mu = exp(out[,mus])
ci <- apply(exp(out[,mus]),2,quantile,c(0.025,0.5,0.975))
pi <- apply(exp(pred),2,quantile,c(0.025,0.5,0.975), na.rm=TRUE)
obs_pi <- apply(pred_obs,2,quantile,c(0.025,0.5,0.975), na.rm=TRUE)

plot(times,ci[2,],type='n',ylim=range(y+.01,na.rm=TRUE), log = "y", ylab="Gloeo count",xlim=times[time.rng])
ciEnvelope(times,obs_pi[1,]+0.0001,obs_pi[3,],col="gray")
ciEnvelope(times,pi[1,],pi[3,],col="Green")
ciEnvelope(times,ci[1,],ci[3,],col="lightBlue")
points(times,y,pch="+",cex=0.5)

```

#Further Diagnostic Checks and Visualization

```{r}

#y vs. preds

obs_diff= vector(mode="numeric", length=0)
obs_quantile = vector(mode="numeric", length=0)
obs_quantile_dm = vector(mode="numeric",length=0)
pred_mean = vector(mode="numeric",length=0)

for(i in 2:ncol(pred)){
  obs_diff[i]=mean(exp(pred[,i]))-y[i] #difference between mean of pred. values and obs for each time point
  pred_mean[i]=mean(exp(pred[,i])) #mean of pred. values at each time point
  percentile <- ecdf(exp(pred[,i])) #create function to give percentile based on distribution of pred. values at each time point
  obs_quantile[i] <- percentile(y[i]) #get percentile of obs in pred distribution
  percentile1 <- ecdf(pred_obs[,i]) #create function to give percentile of obs in distribution of pred including observation error
  obs_quantile_dm[i] <- percentile1(y[i]) #get percentile of obs 
}

#Mean of difference between pred and obs
obspred_mean=mean(obs_diff, na.rm=TRUE)
obspred_mean

#Mean quantile of obs in distribution of pred
obs_quantile_mean = mean(obs_quantile, na.rm = TRUE)
obs_quantile_mean

#Mean quantile of obs in distribution of pred including observation error
obs_quantile_mean_dm = mean(obs_quantile_dm, na.rm = TRUE)
obs_quantile_mean_dm
```

#Plots

```{r}

#hist of quantiles
hist(obs_quantile) #no observation error
hist(obs_quantile_dm, breaks = seq(0,1,0.05)) #with observation error

#plot of mean pred vs. obs
plot(y,pred_mean, xlim = c(0,500), ylim = c(0,500)) #no obs error

## qqplot - plot of quantiles of data in distribution including obs error
plot(seq(0,1,length.out = length(obs_quantile_dm)-1),sort(obs_quantile_dm),
     xlab = "Theoretical Quantile",
     ylab = "Empirical Quantile")
abline(0,1)

## time series 
date=as.character(dat$date)
dates<-as.Date(date)
par(mar = c(5,3,4,3))
plot(dates, obs_quantile_dm,main = "dots = obs. quantiles, triangles = gloeo counts",ylab = "")
mtext("obs. quantile", side=2, line=1.7)
par(new = TRUE)
plot(dates, y, axes = FALSE, bty = "n", xlab = "", ylab = "",pch = 17, col = "red", cex = 0.8)
axis(side=4, at = pretty(range(y)))
mtext("gloeo counts", side=4, line=1.6)

##obs. quantiles vs. temperature
plot(dat$watertemp_mean,obs_quantile_dm)

##obs. quantiles vs. daylength
dat$daylength <- daylength(43.3802, dates)

plot(dat$daylength, obs_quantile_dm)
```

